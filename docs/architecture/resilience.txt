==================
CrateDB Resiliency
==================

Distributed systems are tricky. All sorts of things can go wrong that are
beyond your control. The network can go away, disks can fail, hosts can be
terminated unexpectedly. CrateDB tries very hard to cope with these sorts of
issues while maintaining `availability
<https://crate.io/docs/reference/architecture/shared_nothing.html>`_,
`consistency
<https://crate.io/docs/reference/architecture/storage_consistency.html#consistency>`_,
and `durability
<https://crate.io/docs/reference/architecture/storage_consistency.html# durability>`_.

However, as with any distributed system, sometimes, *rarely*, things can go
wrong.

Thankfully, for most use-cases, if you follow best practices, you are extremely
unlikely to experience resiliency issues with CrateDB.

.. rubric:: Table of Contents

.. contents::
   :local:

Overview
========

Monitoring Cluster Status
-------------------------

.. figure:: resilience-status.png
   :alt:

The Admin UI in CrateDB has a status indicator which can be used to determine
the stability and health of a cluster.

A green status indicates that all shards have been replicated, are available,
and are not being relocated. This is the lowest risk status for a cluster. The
status will turn yellow when there is an elevated risk of encountering issues,
due to a network failure or the failure of a node in the cluster.

The status is updated every few seconds (variable on your cluster `ping
configuration
<https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html>`_).

Storage and Consistency
-----------------------

Code that expects the behavior of an `ACID
<https://en.wikipedia.org/wiki/ACID>`_ compliant database like MySQL may not
always work as expected with CrateDB.

CrateDB does not support ACID transactions, but instead has `atomic operations
<https://crate.io/docs/reference/architecture/storage_consistency.html#atomic-on-document-level>`_
and `eventual consistency
<https://crate.io/docs/reference/architecture/shared_nothing.html>`_ at the row
level. Eventual consistency is the trade-off that CrateDB makes in exchange for
high-availability that can tolerate most hardware and network failures. So you
may observe data from different cluster nodes temporarily falling very briefly
out-of-sync with each other, although over time they will become consistent.

For example, you know a row has been written as soon as you get the ``INSERT
OK`` message. But that row might not be read back by a subsequent ``SELECT`` on
a different node until after a `table refresh
<https://crate.io/docs/reference/sql/refresh.html#refresh-data>`_ (which
typically occurs within one second).

Your applications should be designed to work this storage and consistency model.

Deployment Strategies
---------------------

When deploying CrateDB you should carefully weigh your need for
high-availability and disaster recovery against operational complexity and
expense.

Which strategy you pick is going to depend on the specifics of your situation.

Here are some considerations:

-  CrateDB is designed to scale horizontally. Make sure that your machines are
   fit for purpose, i.e. use SSDs, increase RAM up to 64 GB, and use multiple
   CPU cores when you can. But if you want to dynamically increase (or
   decrease) the capacity of your cluster, `add (or remove) nodes
   <https://crate.io/docs/scale/multi_node_setup/>`_.

-  If availability is a concern, you can add `nodes across multiple zones
   <https://crate.io/docs/scale/multi_zone_setup/>`_ (e.g. different data
   centers or geographical regions). The more available your CrateDB cluster
   is, the more likely it is to withstand external failures like a zone going
   down.

-  If data durability or read performance is a concern, you can increase the
   number of `table replicas
   <https://crate.io/docs/reference/architecture/storage_consistency.html#data-storage>`_.
   More table replicas means a smaller chance of permanent data loss due to
   hardware failures, in exchange for the use of more disk space and more
   intra-cluster network traffic.

-  If disaster recovery is important, you can `take regular snapshots
   <https://crate.io/docs/reference/en/latest/sql/snapshot_restore.html>`_ and
   store those snapshots in cold storage. This safeguards data that has already
   been successfully written and replicated across the cluster.

-  CrateDB works well as part of a `data pipeline
   <https://crate.io/docs/tools/streamsets/>`_, especially if youâ€™re working
   with high-volume data. If you have a message queue in front of CrateDB, you
   can configure it with backups and replay the data flow for a specific
   timeframe. This can be used to recover from issues that affect your data
   before it has been successfully written and replicated across the cluster.

   Indeed, this is the generally recommended way to recover from any of the
   rare consistency or data-loss issues you might encounter when CrateDB
   experiences network or hardware failures (see next section).


Nightly Jepsen tests
--------------------

The source code of these tests can be inspected in the official
`Jepsen repository <https://github.com/jepsen-io/jepsen>`_.


Known Issues
============

CrateDB started of as a fork of Elasticsearch, but over time we have gradually
rewritten large parts of it. And while the execution layer is now completely
unique to CrateDB, the data distribution and replication is still managed by
Elasticsearch.

Most of the known issues that relate to resiliency `exist in the Elasticsearch
layer <https://www.elastic.co/guide/en/elasticsearch/resiliency/current/>`_.

Repeated Cluster Partitions Can Cause Lost Cluster Updates
----------------------------------------------------------

.. raw:: html

   <style>
   table.summary td {
     border: 1px solid black;
     padding: 5px;
   }
   </style>

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Work ongoing (<a
      href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html#_repeated_network_partitions_can_cause_cluster_state_updates_to_be_lost_status_ongoing"
      >More info</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Moderate</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Network issues, unresponsive nodes</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>All</td>
   </tr>
   </table>

.. rubric:: Scenario

A cluster is partitioned and a new master is elected on the side that has
quorum. The cluster is repaired and simultaneously a change is made to the
cluster state. The cluster is partitioned again before the new master node has
a chance to publish the new cluster state and the partition the master lands on
does not have quorum.

.. rubric:: Consequence

The node steps down as master and the uncommunicated state changes are lost.

Cluster state is very important and contains information like shard location,
schemas, and so on. Lost cluster state updates can cause data loss, reset
settings, and problems with table structures.

.. rubric:: Partially fixed


This problem is mostly fixed by `#20384 
<https://github.com/elastic/elasticsearch/pull/20384>`_ (CrateDB v2.0.x),
which uses committed cluster state updates during master election process.
This does not fully solve this rare problem but considerably reduces the chance
of occurrence. The reason is that if the second partition happens concurrently
with a cluster state update and blocks the cluster state commit message from
reaching a majority of nodes, it may be that the in flight update is lost. If
the now-isolated master can still acknowledge the cluster state update to the
client this will result to the loss of an acknowledged change.

Retry of Updates Causes Double Execution
----------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Work ongoing (<a
      href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html#_better_request_retry_mechanism_when_nodes_are_disconnected_status_ongoing"
      >More info</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Moderate</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Network issues, unresponsive nodes</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Non-Idempotent writes</td>
   </tr>
   </table>

.. rubric:: Scenario

A node with a primary shard receives an update, writes it to disk, but goes
offline before having sent a confirmation back to the executing node. When the
node comes back online, it receives an update retry and executes the update
again.

.. rubric:: Consequence

Incorrect data for non-idempotent writes.

For example:

-  An double insert on a table without an explicit primary key would be
   executed twice and would result in duplicate data.

-  A double update would incorrectly increment the row version number twice.

Version Number Representing Ambiguous Row Versions
--------------------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Work ongoing (<a
      href="https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html#_the__version_field_may_not_uniquely_identify_document_content_during_a_network_partition_status_ongoing">More info</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Significant</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Network issues, unresponsive nodes</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Versioned reads with replicated tables while writing.</td>
   </tr>
   </table>

.. rubric:: Scenario

A client is writing to a primary shard. The node holding the primary shard is
partitioned from the cluster. It usually takes between 30 and 60 seconds
(depending on `ping configuration
<https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html>`_)
before the master node notices the partition. During this time, the same row is
updated on both the primary shard (partitioned) and a replica shard (not
partitioned).

.. rubric:: Consequence

There are two different versions of the same row using the same version number.
When the primary shard rejoins the cluster and its data is replicated, the
update that was made on the replicated shard is lost but the new version number
matches the lost update. This will break `Optimistic Concurrency Control
<https://crate.io/docs/reference/sql/occ.html>`_.


Replicas can fall out of sync when a primary shard fails
--------------------------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>
        Initial work to address this issue has been done by introducing
        sequence numbers:
        <a href="https://github.com/elastic/elasticsearch/issues/10708">#10708</a>
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Modest</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Primary fails and in-flight writes are only written to a subset of its replicas</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Writes on replicated table</td>
   </tr>
   </table>

.. rubric:: Scenario

When a primary shard fails, a replica shard will be promoted to be the primary
shard. If there is more than one replica shard, it is possible for the remaining
replicas to be out of sync with the new primary shard. This is caused by
operations that were in-flight when the primary shard failed and may not have
been processed on all replica shards. Currently, the discrepancies are not
repaired on primary promotion but instead would be repaired if replica shards
are relocated (e.g., from hot to cold nodes); this does mean that the length of
time which replicas can be out of sync with the primary shard is
unbounded.

.. rubric:: Consequence

Stale data may be read from replicas.

Fixed Issues
============

Loss of Rows Due to Network Partition
-------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Fixed in Crate v2.0.x (<a
      href="https://github.com/elastic/elasticsearch/issues/7572">#7572</a>, <a
      href="https://github.com/elastic/elasticsearch/issues/14252">#14252</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Significant</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Single node isolation</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Writes on replicated table</td>
   </tr>
   </table>

.. rubric:: Scenario

A node with a primary shard is partitioned from the cluster. The node continues
to accept writes until it notices the network partition. In the meantime,
another shard has been elected as the primary. Eventually, the partitioned node
rejoins the cluster.

.. rubric:: Consequence

Data that was written to the original primary shard on the partitioned node is
lost as data from the newly elected primary shard replaces it when it rejoins
the cluster.

The risk window depends on your `ping configuration
<https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html>`_.
The default configuration of a 30 second ping timeout with three retries
corresponds to a 90 second risk window. However, it is very rare for a node to
lose connectivity within the cluster but maintain connectivity with clients.

Dirty Reads Caused by Bad Primary Handover
------------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Fixed in CrateDB v2.0.x (<a
      href="https://github.com/elastic/elasticsearch/pull/15900">#15900</a>, <a
      href="https://github.com/elastic/elasticsearch/issues/12573">#12573</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Moderate</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Race Condition</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Reads</td>
   </tr>
   </table>

.. rubric:: Scenario

During a primary handover, there is a small risk window when a shard can find
out it has been elected as the new primary before the old primary shard notices
that it is no longer the primary.

A primary handover can happen in the following scenarios:

- A shard is relocated and then elected as the new primary, as two separate but
  sequential actions. Relocating a shard means creating a new shard and then
  deleting the old shard.

- An existing replica shard gets promoted to primary because the primary shard
  was partitioned from the cluster.

.. rubric:: Consequence

Writes that occur on the new primary during the risk window will not be
replicated to the old shard (which still believes it is the primary) so any
subsequent reads on the old shard may return incorrect data.

Changes Are Overwritten by Old Data in Danger of Lost Data
----------------------------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Fixed in CrateDB v2.0.x (<a
      href="https://github.com/elastic/elasticsearch/issues/14671">#14671</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Significant</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Network problems</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Writes</td>
   </tr>
   </table>

.. rubric:: Scenario

A node with a primary that contains new data is partitioned from the cluster.

.. rubric:: Consequence

CrateDB prefers old data over no data, and so promotes an a shard with stale
data as a new primary. The data on the original primary shard is lost. Even if
the node with the original primary shard rejoins the cluster, CrateDB has no
way of distinguishing correct and incorrect data, so that data replaced with
data from the new primary shard.

Make table creation resilient to closing and full cluster crashes
-----------------------------------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>
        The issue has been fixed with the following issues.
        Table recovery: <a href="https://github.com/elastic/elasticsearch/issues/9126">#9126</a>
        Reopening tables: <a href="https://github.com/elastic/elasticsearch/issues/14739">#14739</a>
        Allocation IDs: <a href="https://github.com/elastic/elasticsearch/issues/15281">#15281</a>
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Modest</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very Rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Either the cluster fails while recovering a table or
        the table is closed during shard creation.
    </td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>Table creation</td>
   </tr>
   </table>

.. rubric:: Scenario

Recovering a table requires a quorum of shard copies to be available to allocate
a primary. This means that a primary cannot be assigned if the cluster dies
before enough shards have been allocated. The same happens if a table is closed
before enough shard copies were started, making it impossible to reopen the
table. Allocation IDs solve this issue by tracking allocated shard copies in the
cluster. This makes it possible to safely recover a table in the presence of a
single shard copy. Allocation IDs can also distinguish the situation where a
table has been created but none of the shards have been started. If such an
table was inadvertently closed before at least one shard could be started, a
fresh shard will be allocated upon reopening the table.

.. rubric:: Consequence

The primary shard of the table cannot be assigned or a closed table cannot be
re-opened.

Unaware Master Accepts Cluster Updates
--------------------------------------

.. raw:: html

   <table class="summary">
   <tr>
    <td>Status</td>
    <td>Fixed in CrateDB v2.0.x (<a
      href="https://github.com/elastic/elasticsearch/issues/13062">#13062</a>)
    </td>
   </tr>
   <tr>
    <td>Severity</td>
    <td>Moderate</td>
   </tr>
   <tr>
    <td>Likelihood</td>
    <td>Very rare</td>
   </tr>
   <tr>
    <td>Cause</td>
    <td>Network problems</td>
   </tr>
   <tr>
    <td>Workloads</td>
    <td>DDL statements</td>
   </tr>
   </table>

.. rubric:: Scenario

If a master has lost quorum (i.e. the number of nodes it is in communication
with has fallen below the configured minimum) it should step down as master and
stop answering requests to perform cluster updates. There is a small risk
window between losing quorum and noticing that quorum has been lost, depending
on your `ping configuration
<https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html>`_.

.. rubric:: Consequence

If a cluster update request is made to the node between losing quorum and
noticing the loss of quorum, that request will be confirmed. However, those
updates will be lost because the node will not be able to perform a successful
cluster update.

Cluster state is very important and contains information like shard location,
schemas, and so on. Lost cluster state updates can cause data loss, reset
settings, and problems with table structures.
