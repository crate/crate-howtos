=========================================================================
Building Data Stream pipelines with CrateDB and StreamSets Data Collector
=========================================================================

.. rubric:: Table of Contents

.. contents::
   :local:

Introduction
============

`CrateDB <https://crate.io>`_ can be integrated with `StreamSets
<https://streamsets.com/>`_ using its `JDBC driver
<https://github.com/crate/crate-jdbc>`_ (>= 1.12.1).

The CrateDB JDBC driver can be used as an origin and destination stage in
the StreamSets Data Collector pipeline.

`Install the StreamSets Data Collector
<https://streamsets.com/opensource/>`_ on your localhost or any other
desired location.

In order to build your first data stream pipeline the standalone version
of the CrateDB JDBC driver with included dependencies has to be obtained
and installed in the StreamSets.

You can download the latest standalone version directly from the
`Bintray Repository
<https://bintray.com/crate/crate/crate-jdbc/view/files/io/crate/crate-
jdbc-standalone>`_.

After the JDBC driver is downloaded to the arbitrary destination, you
can proceed with the installation of the driver. We recommend following
the StreamSets instruction for installing `additional drivers
<https://streamsets.com/documentation/datacollector/1.3.0.2/help/index.html#Install_Config/AdditionalDrivers.html>`_. Although to have a quick
start you can simply place the CrateDB JDBC driver jar in the StreamSets
classpath.

The next two sections provide a brief introduction to building simple
data stream pipelines using CrateDB with StreamSets Data Collector. We are
going to demonstrate how to build the pipeline with the `directory
origin
<https://streamsets.com/documentation/datacollector/latest/help/#Origins/Directory.html#concept_qcq_54n_jq>`_ stage which contains CSV data
sample, please refer to the `StreamSets tutorial
<https://streamsets.com/documentation/datacollector/latest/help/#Tutorial/BasicTutorial.html#task_ftt_2vq_ks>`_ to setup the origin
correctly, and stream data to JDBC destination stage using the CrateDB JDBC
driver.

CrateDB JDBC Destination
========================

The CrateDB JDBC Producer destination uses the JDBC connection to write
data to the database. In this section, we show how to build the
StreamSets project for injecting CSV records from the local filesystem
into the CrateDB database with the preliminary preprocessing of certain
records fields.

1. Create the `new pipeline
   <https://streamsets.com/documentation/datacollector/latest/help/#Pipeline_Design/What_isa_Pipeline.html>`_ and follow the above
   mentioned StreamSets `tutorial
   <https://streamsets.com/documentation/datacollector/latest/help/#Tutorial/BasicTutorial.html#task_ftt_2vq_ks>`_ which explains how to
   create the directory origin and read CSV files from the local
   filesystem. Use the New York taxi `data sample
   <https://www.streamsets.com/documentation/datacollector/sample_data/tutorial/nyc_taxi_data.csv>`_ provided in the tutorial.

2. For the sake of simplicity, we use only 4 fields from the CSV file. The
   directory origin does not perform any preliminary preprocessing of
   records. All source fields are represented as strings in the CSV.
   However, some of the selected fields should be processed and
   converted into float values. Therefore, we add an additional
   processing `Field Converter
   <https://streamsets.com/documentation/datacollector/latest/help/#Processors/FieldConverter.html#concept_is3_zkp_wq>`_ stage into the
   pipeline.

   .. image:: 2-streamsets-dst-field-conversion.png
      :alt: Convert string input values

3. Create the taxi table in CrateDB.

   .. code-block:: psql

       CREATE TABLE taxi (
        hack_license STRING,
        medallion STRING,
        total_amount FLOAT,
        tip_amount FLOAT
       );

4. The next step is to configure the CrateDB JDBC destination. First, load the
   CrateDB JDBC driver must be loaded.

   .. image:: 3-streamsets-dst-driver.png
      :alt: Load the CrateDB JDBC driver

   Then, we configure the JDBC connection (1) and define the destination table
   (2). After the above-mentioned configurations steps, we still can see
   warnings in the *Credential tab*. It is solved by unchecking the user
   authorization box because CrateDB does not support an authorization. (3)

   .. image:: 4-streamsets-dst-jdbc-conf.png
      :alt: Configure the CrateDB JDBC driver

5. Start the pipeline.

   .. image:: 5-streamsets-dst-output.png
      :alt: Pipelines run report

CrateDB JDBC Origin
===================

The JDBC Consumer origin uses the JDBC connection to read data from the CrateDB
database. The CrateDB origin stage returns data as a map with column names and
field values.

Currently, the usage of the CrateDB JDBC driver in combination with StreamSets
Data Collector introduces few limitations:

-  Only incremental mode for the data fetch is supported.
-  The offset column should be the primary key column to prevent the insertion
   of duplicate rows.
-  The ``LIMIT`` clause should be explicitly set in the ``SELECT`` query.
-  Batch size in the driver configuration is ignored.

It is important to mention that you have to adjust the ``LIMIT`` size in the
query and the Query Interval property in the JDBC configuration tab to get the
suitable read performance, taking into consideration your particular use case
and the nature of the data.

The followings steps demonstrate how CrateDB can be used as the origin stage in
the data stream pipeline. As sample data, we use `AMPLab rankings dataset
<https://amplab.cs.berkeley.edu/benchmark/>`_. The data can be imported from
`AWS S3 <https://aws.amazon.com/s3/>`_ to the CrateDB database using prepared
data import queries hosted in the `CrateDB demo repository
<https://github.com/crate/crate-demo/tree/master/amplab>`_. Create the
`rankings table
<https://github.com/crate/crate-demo/blob/master/amplab/queries/rankings.sql>`_
and import the `rankings data
<https://github.com/crate/crate-demo/blob/master/amplab/queries/import_rankings_1node.sql>`_. In the demo, we use the dataset which contains 18
millions of records. Assuming that the CrateDB cluster is set up and the
rankings sample data is imported we can start building the data stream pipeline
for streaming the data from the CrateDB database to CSV files.

1. Create the `new pipeline
   <https://streamsets.com/documentation/datacollector/latest/help/#Pipeline_Design/What_isa_Pipeline.html>`_
   and configure the CrateDB JDBC Driver loading as it was done for the JDBC
   destination configuration. In the JDBC tab of the CrateDB JDBC origin we set
   the connection string (1), the SQL query for fetching the data from the
   database (2), the offset value and column (3). For more detail information
   on how to set the offset value and column see the StreamSets `documentation
   <https://streamsets.com/documentation/datacollector/1.1.3/help/#Origins/JDBCConsumer.html#concept_nxz_2kz_bs>`_.

   .. image:: 3-streamsets-source-jdbc.png
      :alt: Configure the CrateDB JDBC driver

   **IMPORTANT**

   To avoid unsupported transaction setting queries that may be invoked
   against CrateDB, it is necessary to uncheck **Enforce Read-only
   Connection** on the advanced tab of the JDBC consumer.

2. We stream the records from CrateDB to CSV files. In order to accomplish that
   we provide the path where the files are going to be created and set the
   output file format in the directory origin to *Delimited*.

   .. image:: 4-streamsets-source-fs-conf1.png
      :alt: Configure CSV destination

3. Now we can start the pipeline and see rankings data streaming statistics
   form the CrateDB database to CSV files.

   .. image:: 6-streamsets-origin-report.png
      :alt: CrateDB to CSV report

-  `Download StreamSets <https://streamsets.com/opensource/>`_
-  `StreamSets Official documentation
   <https://streamsets.com/documentation/datacollector/latest/help/>`_
