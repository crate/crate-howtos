.. _bulk_inserts:

============
Bulk Inserts
============

.. rubric:: Table of Contents

.. contents::
   :local:

Single Inserts
==============

`Single inserts`_ use the ``INSERT`` statement, and look like this:

.. code-block:: psql

    cr> INSERT INTO my_table (column_a) VALUES ("value 1");
    INSERT OK, 1 row affected  (... sec)

Single inserts are typically very fast with CrateDB. A small cluster can
easily handle several thousand inserts per second.

However, it's important to note that:

 1. Every insert is first applied to the primary shard

 2. After the primary shard has been updated, the insert is then individually
    communicated in parallel to every configured replica shard

 3. CrateDB will not return a response until all replica shards have been
    updated

The overhead for each insert (query parsing, planning, and execution, which
includes network connection setups, connection teardowns, and connection
metadata) starts to add up for very heavy insert workloads.

In addition, lots of internal traffic will congest your network, which will
slow down any network-based cluster operations (i.e. inserts, distributed
reads, replication, and cluster management).

Fortunately, there is a solution: bulk inserts.

Switching to Bulk Inserts
=========================

If you can batch up your inserts, switching to *bulk inserts* will dramatically
improve both ingestion performance, and overall cluster performance.

Bulk inserts make use of the CrateDB HTTP endpoint `bulk operations`_ feature
to perform many inserts in a single operation.

Bulk inserts still generate internal network traffic, and CrateDB still waits
until all replicas have been updated before returning a response. But there is
less internal network traffic. In addition, the bulk query only needs to be
parsed, planned, and executed once. And if `translog.durability`_ is set to
``REQUEST`` (the default), a bulk insert only flushes the disk once.

Alternatives
============

If the HTTP endpoint isn't a viable option for you, there are a few
alternatives.

``UNNEST``
----------

CrateDB provides support for bulk inserts via the `UNNEST`_ function.

The ``UNNEST`` function produce rows, like so:

.. code-block:: psql

    cr> SELECT *
    ...   FROM UNNEST(
    ...          [1, 2, 3],
    ...          ['Arthur', 'Trillian', 'Marvin']);
    +------+----------+
    | col1 | col2     |
    +------+----------+
    |    1 | Arthur   |
    |    2 | Trillian |
    |    3 | Marvin   |
    +------+----------+
    SELECT 3 rows in set  (... sec)

Combine ``UNNEST`` with ``INSERT`` to perform bulk inserts:

.. code-block:: psql

    cr> INSERT INTO my_table (id, name)
    ...   (SELECT *
    ...      FROM UNNEST(
    ...             [1, 2, 3],
    ...             ['Arthur', 'Trillian', 'Marvin']));
    INSERT OK, 3 rows affected  (... sec)

This method of doing bulk inserts should perform about as well as the HTTP
endpoint method, and is therefore the recommended alternative.

If your client supports query string parameter substitution, you can use the
``UNNEST`` method with static prepared statements.

For example, using the CrateDB Python client, the following is possible:

.. code-block:: python

    client.execute("""
      INSERT INTO my_table (id, name)
        (SELECT *
           FROM UNNEST(?, ?))
    """, ([1, 2, 3], ["Arthur", "Trillian", "Marvin"]))

Here, you can vary the number of rows being inserted without having to change
the prepared statement.

Multiple Value Expressions
--------------------------

You can insert multiple rows with multiple value expressions, like so:

.. code-block:: psql

    cr> INSERT INTO my_table (id, name)
    ...      VALUES (1, 'Arthur'),
    ...             (2, 'Trillian'),
    ...             (2, 'Marvin');
    INSERT OK, 3 rows affected  (... sec)

This method of doing bulk inserts is usually slower than the ``UNNEST`` method,
because parsing is more expensive. The query looks nicer for humans though.

The only problem is that the structure of the insert statement is variable on
the number of rows to insert. So, if you are inserting a variable number of
rows, you have to prepare the SQL statement using some form of string
concatenation each time.

Query string parameter substitution is recommended over string concatenation,
and so the ``UNNEST`` method is recommended over the multiple value expressions
method.

Client Feature
--------------

Some clients offer their own bulk inserts implementation.

`The JDBC client`_, for instance, provides the `addBatch`_ and `executeBatch`_
methods.

Typically, these implementations still use individual insert statements, but
they have a lower resource overhead because they use the binary protocol,
contain almost no headers, and are executed over an already established
connection.

The client batching method usually performs better than manual single inserts,
but worse than both the ``UNNEST`` method and multiple value expressions method.

Performance Testing
===================

Every setup is different. The best way to tune your particular setup is to do
performance testing.

The easiest way to do this is to use the `cr8`_ tool.

First of all, create a table specifically for performance testing:

.. code-block:: psql

    cr> CREATE TABLE my_table (
    ...   id integer,
    ...   name string
    ... );
    CREATE OK, 1 row affected  (... sec)

This is a simple example. You should create a table that mirrors the sort of
data you plan to use in production.

Next, generate some fake data:

.. code-block:: sh

    sh$ cr8 insert-fake-data \
          --hosts localhost:4200 \
          --table my_table \
          --num-records 1000000

This command will look at your table schema, generate appropriate test data for
the schema, and insert one million test records. You can adjust the number of
records if you wish.

It's important to generate the fake data as a separate step so that our
performance testing isn't also measuring the fake data generation, which in
some situations, might actually end up being the performance bottleneck.

Now, export the the fake data as JSON:

.. code-block:: psql

    cr> COPY my_table TO DIRECTORY "/tmp/crate"
    COPY OK, 1000000 rows affected  (... sec)

We're exporting to the ``/tmp/crate`` directory here, but you can export to any
directory you choose.

Truncate the table:

.. code-block:: psql

    cr> DELETE FROM my_table;
    DELETE OK, 1000000 rows affected  (... sec)

You can then insert the fake data again and measure performance, like so:

.. code-block:: sh

    sh$ cat /tmp/crate/my_table_*.json | cr8 insert-json \
          --hosts localhost:4200 \
          --table my_table \
          --bulk-size 1000 \
          --concurrency 25

.. NOTE::

   The ``--bulk-size`` and ``--concurrency`` values in the above example are
   set to the default values. If you omit these flags, this is the
   configuration that will be used.

The ``insert-json`` command should produce data like this::

    Executing inserts: bulk_size=1000 concurrency=25
    1000 requests [00:35, 27.84 requests/s]
    Runtime (in ms):
        mean:    103.556 ± 3.957
        min/max: 11.587 → 521.434
    Percentile:
        50:   89.764 ± 63.851 (stdev)
        95:   220.739
        99.9: 475.568

From here, you can adjust the configuration values, and compare the results to
understand the performance profile of your setup.

.. NOTE:

   Setting the bulk records size to `1` approximates the performance of single
   inserts.

.. _benchmarking: https://crate.io/a/insert-boost-on-replicas/
.. _single inserts: https://crate.io/docs/crate/reference/sql/dml.html#inserting-data
.. _SQL HTTP endpoint: https://crate.io/docs/crate/reference/protocols/http.html
.. _bulk operations: https://crate.io/docs/crate/reference/protocols/http.html#bulk-operations
.. _UNNEST: https://crate.io/docs/crate/reference/en/latest/sql/table_functions.html#unnest-array-array
.. _cr8: https://github.com/mfussenegger/cr8/
.. _translog.durability: https://crate.io/docs/crate/reference/en/latest/sql/reference/create_table.html#translog-durability
.. _the JDBC client: https://crate.io/docs/clients/jdbc/en/latest/
.. _addBatch: https://docs.oracle.com/javase/7/docs/api/java/sql/Statement.html#addBatch(java.lang.String)
.. _executeBatch: https://docs.oracle.com/javase/7/docs/api/java/sql/Statement.html#executeBatch()
