.. _insert_architecture:

===================
Insert Architecture
===================

If you want to optimize your use of CrateDB for insert performance, it's
important that you have a basic understanding of how CrateDB handles inserts
internally.

.. rubric:: Table of Contents

.. contents::
   :local:

.. _types_of_insert:

Types of Insert
===============

There are three kinds of inserts as far as CrateDB's insert handling is
concerned.

Single Inserts
--------------

A single insert looks like this:

.. code-block:: psql

  cr> INSERT INTO my_table (id, first_name)
  ...      VALUES (1, "Arthur");
  INSERT OK, 1 row affected  (... sec)

Inserts With Multiple Value Expressions
---------------------------------------

An insert with :ref:`multiple value expressions <bulk_multiple_values>` looks
like this:

.. code-block:: psql

   cr> INSERT INTO my_table (id, first_name)
   ...      VALUES (1, 'Arthur'),
   ...             (2, 'Trillian'),
   ...             (2, 'Marvin');
   INSERT OK, 3 rows affected  (... sec)

Bulk Inserts
------------

Bulk inserts are typically done via :ref:`the CrateDB HTTP endpoint
<bulk_switching>`, but they can also be done with :ref:`client batching
<bulk_client>` or with SQL via :ref:`UNNEST <bulk_unnest>`. Consult the
respective docs for details of each method.

All three bulk insert methods are handled the same internally by CrateDB.

.. _processing_phases:

Processing Phases
=================

.. _parsing_analyzing:

Parsing and Analyzing
---------------------

When a user issues an insert statement to the cluster then this request is sent
to whichever individual node the client connected to. This node is then
responsible for performing statement parsing, analyzing, and planning.

The statement is parsed by the *SQL parser*. The *parsed statement* is then
analyzed by the *statement analyzer*, which produces an *analyzed statement*.

The statement analyzer performs the following checks:

- Does the target table exist?

- Do the specified table columns exist?

   - If not, does the table have a `dynamic column policy`_?

- Do the data types of the insert values match the data types of the existing
  columns?

- Are all ``PRIMARY KEY`` columns and ``NOT NULL`` column specified?

- Are the values for ``NOT NULL`` columns non null?

- Is the table partitioned?

  - If so, does a `partition`_ exist for all insert values?

    - If not, enrich the statement with a list of the partitions that need to
      be created

.. _planning:

Planning
--------

The analyzed statement is then handed off to the *planner*, which plans out the
execution of the insert. This process continues to take place on the node
responsible for handling the client request.

The planner will create one more more ``InsertRequest`` objects. Each
``InsertRequest`` object holds the details of one row to be inserted.

If there are multiple inserts, the planner will also create a `hashmap`_
that groups ``InsertRequest`` objects by shard ID (``ShardId``). We will use
Java synatx and refer to this, like so: ``Map<ShardId, List<InsertRequest>>``.

How the planner proceeds depends on the type of insert being prepared.

Planning Single Inserts
.......................

The planner performs the following actions:

1. If the partition doesn't exist, create a new partition

2. Create a single ``InsertRequest`` object

Planning Inserts With Multiple Value Expressions
................................................

For each row to be inserted, the planner performs the following actions:

1. If the partition doesn't exist, create a new partition

2. Calculate the shard ID (``ShardId``) using the `routing column`_

3. Create an ``InsertRequest`` object and add it to ``Map<ShardId,
   List<InsertRequest>>``

Planning Bulk Inserts
.....................

The planner performs the same steps taken for inserts with multiple value
expressions, as listed under the previous header.

Additionally, the planner creates an ``BulkResponse`` object, which holds the
fine-grain response data so that we can start populate it while processing the
individual rows to be inserted, we proceed with the actual write operations:

.. _execution:

Execution
---------

The planned statement is then handed off to the *executor*, which executes the
insert across the cluster.

This process is managed by the node responsible for handling the client
request, but the operations needed to complete the insert can be handed off to
any `data node`_ in the cluster.

How the executor proceeds depends on the type of insert being executed.

Executing Single Inserts
........................

The executor performs the following actions:

1. Query the :ref:`cluster state <cluster_state_management>` for the the node
   that holds the primary shard corresponding to the ``shardId`` of the
   ``InsertRequest``

2. Check that :ref:`the required number of replicas <minimum_active_shards>`
   are available

3. Send a write request corresponding to the ``InsertRequest`` to the primary
   shard

4. Send an identical write request to the replicas of the primary shard

   - If successful, a response is sent to the client reporting that one row has
     been inserted

   - Otherwise, an error (when using HTTP) or an exception (when using the
     Postgres wire protocol) is sent to the client

Executing Inserts With Multiple Value Expressions
.................................................

The executor makes a best effort to insert as many rows as possible. The
executor will continue to try to insert rows, even when errors are encountered.

The ``Map<ShardId, List<InsertRequest>>`` hashmap is processed one ``shardId``
and ``List<InsertRequest>`` object pair at a time.

For each pair, the executor performs the following actions:

1. Query the :ref:`cluster state <cluster_state_management>` for the the node
   that holds the primary shard corresponding to the ``shardId``

2. Check that :ref:`the required number of replicas <minimum_active_shards>`
   are available

4. Send a write request corresponding to the ``List<InsertRequest>`` to the
   primary shard

5. Send an identical write request to the replicas of the primary shard

   - If successful, an insert counter is incremented

6. A response is sent to the user reporting the value of the successful insert
   counter

.. NOTE::

   If an insert request with multiple value expressions results in no
   successful inserts, this is reported like any other result and does not
   produce an an error (when using HTTP) or an exception (when using the
   Postgres wire protocol).

Executing Bulk Inserts
......................

The executor performs the same steps taken for inserts with multiple value
expressions, as listed under the previous header, with one exception.

Instead of reporting a single number corresponding to the number of successful
inserts, the previously created ``BulkResponse`` object is populated with
information about every insert attempted.

The ``BulkResponse`` is then used to create a `bulk insert response`_ that is
returned to the client, with a ``rowcount`` for every row in the bulk insert
request:

- A ``rowcount`` of ``1`` indicates a successful insert

- A ``rowcount`` of ``2`` indicates a failed insert, and may be
  accompanied with with an ``error_message``

.. _minimum_active_shards:

Minimum Active Shards
=====================

The `write.wait_for_active_shards`_ table setting configures the minimum number
of shards that must be available for inserts to take place:

- If set to ``all`` (the default value) the primary shard and all replica
  shards must be available

- If set to some value ``x``, the insert will only proceed once the primary
  shard plus ``x-1`` replica shards are available

- If set to ``1``, the insert will proceed if the primary shard is available

If the required number of replicas is not available, and do not become
available during a brief waiting period, the operation fails and an error is
returned back to the user.

TODO IS THE WAITING PERIOD CONFIGURABLE?

TODO the ES docs say this improves resiliency. but why? surely the replication
notices missing copies of the data on the replicas and will fix the issue after
the insert takes place eventually?

.. SEEALSO::

   `Elasticsearch documentation`_

.. _dynamic column policy: https://crate.io/docs/crate/reference/en/latest/sql/ddl/column_policy.html#dynamic
.. _primary key: https://crate.io/docs/crate/reference/en/latest/sql/ddl/constraints.html#primary-key
.. _not null: https://crate.io/docs/crate/reference/en/latest/sql/ddl/constraints.html#not-null
.. _partition: https://crate.io/docs/crate/reference/en/latest/sql/partitioned_tables.html
.. _routing column: https://crate.io/docs/crate/reference/en/latest/sql/ddl/sharding.html#routing
.. _hashmap: https://en.wikipedia.org/wiki/Hash_table
.. _bulk operations: https://crate.io/docs/crate/reference/protocols/http.html#bulk-operations
.. _more info: https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html#index-wait-for-active-shards
.. _data node: https://crate.io/docs/crate/reference/en/latest/configuration/node.html#node-types
.. _bulk insert response: https://crate.io/docs/crate/reference/en/latest/protocols/http.html#bulk-errors
.. _write.wait_for_active_shards: https://crate.io/docs/crate/reference/en/latest/sql/reference/create_table.html#sql-ref-write-wait-for-active-shards
.. _Elasticsearch documentation: https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-index_.html#index-wait-for-active-shards
